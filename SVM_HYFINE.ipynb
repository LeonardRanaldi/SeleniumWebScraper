{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_HYFINE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_2VsAvi_T26j",
        "QuKM3tENvQNl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VsAvi_T26j",
        "colab_type": "text"
      },
      "source": [
        "# Plotting performances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Eb1xG8zVby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, log_loss, accuracy_score, confusion_matrix\n",
        "\n",
        "def plot_cm(ax, y_true, y_pred, classes, title, th=0.5, cmap=plt.cm.Blues):\n",
        "    y_pred_labels = (y_pred>th).astype(int)\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred_labels)\n",
        "    \n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.set_title(title)\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    ax.set_xticks(tick_marks)\n",
        "    ax.set_yticks(tick_marks)\n",
        "    ax.set_xticklabels(classes)\n",
        "    ax.set_yticklabels(classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        ax.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    ax.set_ylabel('True label')\n",
        "    ax.set_xlabel('Predicted label')\n",
        "\n",
        "def plot_auc(ax, y_train, y_train_pred, y_test, y_test_pred, th=0.5):\n",
        "\n",
        "    y_train_pred_labels = (y_train_pred>th).astype(int)\n",
        "    y_test_pred_labels  = (y_test_pred>th).astype(int)\n",
        "\n",
        "    fpr_train, tpr_train, _ = roc_curve(y_train,y_train_pred)\n",
        "    roc_auc_train = auc(fpr_train, tpr_train)\n",
        "    acc_train = accuracy_score(y_train, y_train_pred_labels)\n",
        "\n",
        "    fpr_test, tpr_test, _ = roc_curve(y_test,y_test_pred)\n",
        "    roc_auc_test = auc(fpr_test, tpr_test)\n",
        "    acc_test = accuracy_score(y_test, y_test_pred_labels)\n",
        "\n",
        "    ax.plot(fpr_train, tpr_train)\n",
        "    ax.plot(fpr_test, tpr_test)\n",
        "\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('ROC curve')\n",
        "    \n",
        "    train_text = 'train acc = {:.3f}, auc = {:.2f}'.format(acc_train, roc_auc_train)\n",
        "    test_text = 'test acc = {:.3f}, auc = {:.2f}'.format(acc_test, roc_auc_test)\n",
        "    ax.legend([train_text, test_text])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuKM3tENvQNl",
        "colab_type": "text"
      },
      "source": [
        "# Reading File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HohDgx3gvNln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import json\n",
        "\n",
        "def readFile(name):\n",
        "    d = collections.defaultdict(dict)\n",
        "    with open(name) as f:\n",
        "        d = json.load(f)\n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4WvPX3_vZbz",
        "colab_type": "text"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVDXILN0Ef9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "final_dataset = readFile('name_dataset.json')\n",
        "\n",
        "df = pd.DataFrame.from_dict(final_dataset, orient='index', columns=['Name_Similarity', 'Img_Similarity', 'Country_Similarity', 'Gender_Similarity', 'Class'])\n",
        "# delete null cells\n",
        "df = df.dropna()\n",
        "# shape dataset\n",
        "df.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91vIqQRnZyeE",
        "colab_type": "text"
      },
      "source": [
        "# Define Values, Targets and Splitting dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlIA0ONeZ5sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#delete columns \n",
        "#X = df.drop(['Col1', 'Col2'], axis=1)\n",
        "\n",
        "#define X\n",
        "X = df.drop(['Class'], axis=1)\n",
        "#define y\n",
        "y = df['Class']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vbg6P2UP418",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X.shape,y.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hR7yQqozlHr",
        "colab_type": "text"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ3STk4OzquK",
        "colab_type": "code",
        "outputId": "318de54f-7005-4378-f522-fe38fe7d66b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}] # kernel rbf con parametri + kernel lineare con parametri \n",
        "\n",
        "\n",
        "clf = GridSearchCV(SVC(class_weight='balanced'), tuned_parameters, cv=2,\n",
        "                       scoring='f1_macro', verbose=5)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "y_train_pred = clf.predict(X_train)\n",
        "y_test_pred = clf.predict(X_test)\n",
        "\n",
        "y_train_pred_labels = y_train_pred\n",
        "y_test_pred_labels  = y_test_pred\n",
        "\n",
        "print(\"Best params: \")\n",
        "print(clf.best_params_)\n",
        "\n",
        "\n",
        "acc_train = accuracy_score(y_train, y_train_pred_labels)\n",
        "\n",
        "print(\"Train Classification Report: \\n\")\n",
        "print(classification_report(y_train, y_train_pred_labels))\n",
        "print(\"Accuracy Train: \" + str(acc_train) + \"\\n\")\n",
        "\n",
        "print(\"Test Classification Report: \\n\")\n",
        "print(classification_report(y_test, y_test_pred_labels))\n",
        "\n",
        "\n",
        "acc_test = accuracy_score(y_test, y_test_pred_labels)\n",
        "\n",
        "print(\"Accuracy Test: \" + str(acc_test) + \"\\n\")\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "fig,ax = plt.subplots(1,3)\n",
        "fig.set_size_inches(15,5)\n",
        "\n",
        "plot_cm(ax[0],  y_train, y_train_pred, [0,1], 'Confusion matrix (TRAIN)', threshold)\n",
        "plot_cm(ax[1],  y_test, y_test_pred,   [0,1], 'Confusion matrix (TEST)', threshold)\n",
        "plot_auc(ax[2], y_train, y_train_pred, y_test, y_test_pred, threshold)\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntuned_parameters = [{\\'kernel\\': [\\'rbf\\'], \\'gamma\\': [1e-3, 1e-4], \\'C\\': [1, 10, 100, 1000]}, {\\'kernel\\': [\\'linear\\'], \\'C\\': [1, 10, 100, 1000]}] # kernel rbf con parametri + kernel lineare con parametri \\n\\n\\nclf = GridSearchCV(SVC(class_weight=\\'balanced\\'), tuned_parameters, cv=2,\\n                       scoring=\\'f1_macro\\', verbose=5)\\nclf.fit(X_train, y_train)\\n\\n\\n\\ny_train_pred = clf.predict(X_train)\\ny_test_pred = clf.predict(X_test)\\n\\ny_train_pred_labels = y_train_pred\\ny_test_pred_labels  = y_test_pred\\n\\nprint(\"Best params: \")\\nprint(clf.best_params_)\\n\\n\\nacc_train = accuracy_score(y_train, y_train_pred_labels)\\n\\nprint(\"Train Classification Report: \\n\")\\nprint(classification_report(y_train, y_train_pred_labels))\\nprint(\"Accuracy Train: \" + str(acc_train) + \"\\n\")\\n\\nprint(\"Test Classification Report: \\n\")\\nprint(classification_report(y_test, y_test_pred_labels))\\n\\n\\nacc_test = accuracy_score(y_test, y_test_pred_labels)\\n\\nprint(\"Accuracy Test: \" + str(acc_test) + \"\\n\")\\n\\nthreshold = 0.5\\n\\nfig,ax = plt.subplots(1,3)\\nfig.set_size_inches(15,5)\\n\\nplot_cm(ax[0],  y_train, y_train_pred, [0,1], \\'Confusion matrix (TRAIN)\\', threshold)\\nplot_cm(ax[1],  y_test, y_test_pred,   [0,1], \\'Confusion matrix (TEST)\\', threshold)\\nplot_auc(ax[2], y_train, y_train_pred, y_test, y_test_pred, threshold)\\n    \\nplt.tight_layout()\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}